{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependencies\n",
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install datasets\n",
    "#!pip3 install evaluate\n",
    "#!pip3 install gradio\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install torch\n",
    "#!pip3 install torchvison\n",
    "#!pip3 install torchaudio\n",
    "#!pip3 install transformers\n",
    "#!pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c772b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import torch\n",
    "import torchvision\n",
    "import wget\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from torchvision.transforms import RandomResizedCrop\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from transformers import AutoModelForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_API_URL = f'https://github.com/jonfernandes/flowers-dataset/raw/main/flower_photos.tgz'\n",
    "\n",
    "DATA_DIR = '/Volumes/Data 1'\n",
    "HUGGINGFACE_FILE_NAME = 'huggingface'\n",
    "\n",
    "DATASET_FILE_NAME = 'flower_images_dataset'\n",
    "DATASET_FILE_PATH = os.path.join(DATA_DIR, HUGGINGFACE_FILE_NAME, DATASET_FILE_NAME)\n",
    "\n",
    "#ImageNet Model with 1000 pre-trained classes of images\n",
    "MODEL_ID = 'google/vit-base-patch16-224'\n",
    "MODEL_FILE_PATH = os.path.join(DATA_DIR, HUGGINGFACE_FILE_NAME)\n",
    "\n",
    "DEVICE = ''\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "METRIC_NAME = 'accuracy'\n",
    "MODEL_NAME = 'vit-base-path16-224-finetuned-flower'\n",
    "FINETUNED_MODEL_FILE_PATH = os.path.join(DATA_DIR, HUGGINGFACE_FILE_NAME, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a86471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accleration Hardware Selection\n",
    "\n",
    "# FOR PC PyTorch CUDA GPU training acceleration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "# For Mac PyTorch Metal Performance Shaders (MPS) GPU training acceleration\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "\n",
    "# Default to CPU training\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4443867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading In Dataset\n",
    "dataset = load_dataset('imagefolder', data_files = DATASET_API_URL, cache_dir = DATASET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ffdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    display(dataset['train'][i]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f04f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ab70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_validation = dataset['train'].train_test_split(test_size = 0.1, seed = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa82057",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_validation['validation'] = dataset_train_validation.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a133274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update(dataset_train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test = dataset['train'].train_test_split(test_size = 0.1, seed = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cad78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update(dataset_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320952bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing a pre-trained model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(pretrained_model_name_or_path = MODEL_ID, \n",
    "                                                        cache_dir = MODEL_FILE_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(pretrained_model_name_or_path = MODEL_ID, cache_dir = MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200298ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_id = 3\n",
    "test_image = dataset['train'][training_image_id]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a57ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numpy_representation = feature_extractor(images = test_image, return_tensors = 'pt').to(DEVICE)\n",
    "output = model(**image_numpy_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeled logits of the 1000 potential classes the input image could be\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest output class\n",
    "torch.argmax(output.logits, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14930c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label for the input image\n",
    "prediction = torch.argmax(output.logits, dim = 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 738 is mapped to the label of flowepot\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between the IDs and labels\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682472cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    print(f\"'{label}' label is present within the pre-trained model: {label in model.config.label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining A Model\n",
    "id2label = {key: value for key, value in enumerate(labels)}\n",
    "label2id = {value:key for key, value in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a59855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ID:Label Mapping - {id2label}')\n",
    "print(f'Label:ID Mapping - {label2id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pre-trained model to suit the current usecase, giving it the new number of labels,\n",
    "# mapping of these labels to their appropriate IDs and vice verse, and to ignore the fact that it\n",
    "# had previously been trained to distinguish between 1000 classes of objects\n",
    "model = AutoModelForImageClassification.from_pretrained(pretrained_model_name_or_path = MODEL_ID, \n",
    "                                                        cache_dir = MODEL_FILE_PATH,\n",
    "                                                        num_labels = len(labels),\n",
    "                                                        id2label = id2label,\n",
    "                                                        label2id = label2id,\n",
    "                                                        ignore_mismatched_sizes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4121bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the images to avoid issues with extremes and outliers\n",
    "normalize = Normalize(mean = feature_extractor.image_mean, std = feature_extractor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd22977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected image dimensions\n",
    "feature_extractor.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b828f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(feature_extractor.size.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(feature_extractor.size.values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8725dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "# Applying various random transformations to a limited set of images from the flowers dataset to produce\n",
    "# a greater variety of 'new' images that can be trained upon, with the images now being bigger, smaller, \n",
    "# rotated every which way, lighter, darker, etc.\n",
    "training_transformation = Compose([\n",
    "    RandomResizedCrop(list(feature_extractor.size.values())[0]),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# For the validation portion of the dataset, these images are then placed back into their proper formatting\n",
    "# as inputs that will be fed into the model, meaning orienting them correctly, resizing them to an appropriate\n",
    "# resolution, etc.\n",
    "validation_transformation = Compose([\n",
    "        Resize(list(feature_extractor.size.values())[0]),\n",
    "        CenterCrop(list(feature_extractor.size.values())[0]),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_training_images(images):\n",
    "    images[\"pixel_values\"] = [training_transformation(image.convert(\"RGB\")) for image in images[\"image\"]]\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73615ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_validation_images(images):\n",
    "    images[\"pixel_values\"] = [validation_transformation(image.convert(\"RGB\")) for image in images[\"image\"]]\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = dataset.with_transform(transform_training_images)\n",
    "transformed_dataset['train'] = dataset['train'].with_transform(transform_training_images)\n",
    "transformed_dataset['validation'] = dataset['validation'].with_transform(transform_validation_images)\n",
    "transformed_dataset['test'] = dataset['test'].with_transform(transform_validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_image = training_transformation(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensor format: (channels, rows, columns)\n",
    "# Matplotlib tensor format: (rows, columns, channels)\n",
    "# Rearranging of the test image's tensor's format is require to avoid an error\n",
    "plt.imshow(transformed_test_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd734d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_image = validation_transformation(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d140a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformed_test_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fromatting images for input\n",
    "four_test_images = [transformed_dataset['train'][i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fca96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "four_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c71e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_test_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(four_test_images))\n",
    "print(type(four_test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in four_test_images:\n",
    "    print(image['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_test_images_labels = [image['label'] for image in four_test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_test_images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9843a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels must be formatted into the PyTorch tensor type\n",
    "four_test_images_labels = torch.tensor([image['label'] for image in four_test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_test_images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919add82",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_test_images_pixel_values = torch.stack([image['pixel_values'] for image in four_test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct input shape for tensorflow is (batch_size, number_of_channels, height_in_pixels, width_in_pixels)\n",
    "four_test_images_pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c074631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collating images together for batch processing\n",
    "def batch_collate_images(images):\n",
    "    labels = torch.tensor([image['label'] for image in images])\n",
    "    pixel_values = torch.stack([image['pixel_values'] for image in images])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    transformed_dataset['train'],\n",
    "    batch_size = 4,\n",
    "    collate_fn = batch_collate_images,\n",
    "    shuffle = True)\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    transformed_dataset['validation'],\n",
    "    batch_size = 4,\n",
    "    collate_fn = batch_collate_images,\n",
    "    shuffle = False)\n",
    "\n",
    "testing_dataloader = DataLoader(\n",
    "    transformed_dataset['test'],\n",
    "    batch_size = 4,\n",
    "    collate_fn = batch_collate_images,\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d9779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in batch.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_arguments = TrainingArguments(\n",
    "    MODEL_NAME,\n",
    "    evaluation_strategy = 'steps',\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = 5,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = METRIC_NAME,\n",
    "    remove_unused_columns = False,\n",
    "    #logging_dir = './logs',\n",
    "    push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172728ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric = evaluate.load(METRIC_NAME, cache_dir = DATASET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(batch):\n",
    "    return evaluation_metric.compute(\n",
    "        references = batch.label_ids,\n",
    "        predictions = np.argmax(batch.predictions, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a19962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = Trainer(\n",
    "    model = model,\n",
    "    args = model_training_arguments,\n",
    "    train_dataset = transformed_dataset['train'],\n",
    "    eval_dataset = transformed_dataset['validation'],\n",
    "    tokenizer = feature_extractor,\n",
    "    data_collator = batch_collate_images,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c953761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainOutput\n",
    "# global_step = 279\n",
    "# training_loss = 0.14041148633512546\n",
    "# metrics = {'train_runtime': 1247.4732, 'train_samples_per_second': 7.147, 'train_steps_per_second': 0.224, \n",
    "# 'total_flos': 6.909371568577659e+17, 'train_loss': 0.14041148633512546, 'epoch': 3.0}\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff94066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.predict(transformed_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328eb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_loss: 0.051226019859313965\n",
    "# eval_runtime: 165.6979\n",
    "# eval_samples_per_second: 17.936\n",
    "# eval_steps_per_second: 0.561\n",
    "# epoch: 3.0\n",
    "model_trainer.evaluate(transformed_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_loss: 0.0882396474480629\n",
    "# eval_runtime: 20.3162\n",
    "# eval_samples_per_second: 18.064\n",
    "# eval_steps_per_second: 0.591\n",
    "# epoch: 3.0\n",
    "model_trainer.evaluate(transformed_dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a05ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_loss: 0.0770476683974266\n",
    "# eval_runtime: 20.3162\n",
    "# eval_samples_per_second: 17.326\n",
    "# eval_steps_per_second: 0.576\n",
    "# epoch: 3.0\n",
    "model_trainer.evaluate(transformed_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c98ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.save_model(FINETUNED_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ca393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing argmax will return only the largest value associated with a class of flower\n",
    "# Softmax, on the other hand, will return all of the associated probabilities for all classes of flowers\n",
    "def classify_image(image):\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_FILE_PATH)\n",
    "    model = AutoModelForImageClassification.from_pretrained(FINETUNED_MODEL_FILE_PATH).to('mps')\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(FINETUNED_MODEL_FILE_PATH)\n",
    "    inputs = feature_extractor(image, return_tensors = 'pt').to('mps')\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Argmax Approach\n",
    "    #predictions = torch.argmax(outputs.logits, dim = -1).item()\n",
    "    #return model.config.id2label[predictions]\n",
    "    \n",
    "    # Softmax Approach\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
    "    predictions = predictions[0].cpu().detach().numpy()\n",
    "    confidences = {label: float(predictions[i]) for i, label in enumerate(labels)}\n",
    "    return confidences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image2 = dataset['test'][-1]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cac3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_image(test_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FINETUNED_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04440a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs and weights must be on the same device\n",
    "model = AutoModelForImageClassification.from_pretrained(FINETUNED_MODEL_FILE_PATH).to('cpu')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(FINETUNED_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = pipeline('image-classification', model = model, feature_extractor = feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier(test_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc5185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
